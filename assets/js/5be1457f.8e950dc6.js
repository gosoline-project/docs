"use strict";(self.webpackChunkgosoline_docs=self.webpackChunkgosoline_docs||[]).push([["4919"],{9396(e){e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"identity-and-naming-patterns","metadata":{"permalink":"/docs/blog/identity-and-naming-patterns","editUrl":"https://github.com/gosoline-project/docs/tree/main/blog/2026-02-19-identity-and-naming-patterns.md","source":"@site/blog/2026-02-19-identity-and-naming-patterns.md","title":"Identity & Naming Patterns: Flexible, Configuration-Driven Resource Naming","description":"Managing cloud resources across multiple environments, teams, and services is hard enough without fighting your own naming conventions. Gosoline\'s old AppId model locked you into a fixed project/family/group hierarchy \u2014 useful in practice, but inflexible by design. This release replaces it with a tag-based Identity system and a unified naming pattern engine that gives you full control over how every resource is named, from a single config file.","date":"2026-02-19T00:00:00.000Z","tags":[{"inline":true,"label":"identity","permalink":"/docs/blog/tags/identity"},{"inline":true,"label":"naming","permalink":"/docs/blog/tags/naming"},{"inline":true,"label":"configuration","permalink":"/docs/blog/tags/configuration"},{"inline":true,"label":"breaking-changes","permalink":"/docs/blog/tags/breaking-changes"}],"readingTime":4.93,"hasTruncateMarker":true,"authors":[{"name":"Jan Kamieth","url":"https://github.com/j4k4","imageURL":"https://avatars.githubusercontent.com/u/783502?s=400&v=4","key":"jaka","page":null}],"frontMatter":{"slug":"identity-and-naming-patterns","title":"Identity & Naming Patterns: Flexible, Configuration-Driven Resource Naming","authors":["jaka"],"tags":["identity","naming","configuration","breaking-changes"]},"unlisted":false,"nextItem":{"title":"Log Sampling in Go: Less Noise, More Debuggability","permalink":"/docs/blog/sampling-fingers-crossed-logging"}},"content":"Managing cloud resources across multiple environments, teams, and services is hard enough without fighting your own naming conventions. Gosoline\'s old `AppId` model locked you into a fixed `project/family/group` hierarchy \u2014 useful in practice, but inflexible by design. This release replaces it with a tag-based `Identity` system and a unified naming pattern engine that gives you full control over how every resource is named, from a single config file.\\n\\n{/* truncate */}\\n\\n## What changed at a glance\\n\\n- The fixed `AppId` fields (`project`, `family`, `group`) are replaced by a flexible `app.tags` map \u2014 define any keys your organization needs.\\n- A new **unified naming pattern engine** controls the names of SQS queues, SNS topics, DynamoDB tables, Kinesis streams, Kafka topics, Redis keys, CloudWatch namespaces, Prometheus prefixes, and tracing service names \u2014 all from config.\\n- `{app.namespace}` lets you define your naming hierarchy once and reference it everywhere.\\n- `application.Default()` has been slimmed down from ~22 options to 7, making every capability an explicit opt-in.\\n- `pkg/es/` (Elasticsearch) and `pkg/parquet/` have been removed.\\n\\n---\\n\\n## The new Identity model\\n\\n### Before and after\\n\\nThe old flat top-level config keys are replaced with a nested `app:` block:\\n\\n```yaml\\n# Before\\nenv: production\\napp_project: myproject\\napp_family: platform\\napp_group: core\\napp_name: my-service\\n\\n# After\\napp:\\n  env: production\\n  name: my-service\\n  namespace: \\"{app.tags.project}.{app.env}.{app.tags.family}.{app.tags.group}\\"\\n  tags:\\n    project: myproject\\n    family: platform\\n    group: core\\n```\\n\\nIf you set your config via environment variables, the keys change too:\\n\\n| Old variable | New variable |\\n| :--- | :--- |\\n| `APP_PROJECT` | `APP_TAGS_PROJECT` |\\n| `APP_FAMILY` | `APP_TAGS_FAMILY` |\\n| `APP_GROUP` | `APP_TAGS_GROUP` |\\n| `APP_NAME` | `APP_NAME` (unchanged) |\\n| `ENV` | `APP_ENV` |\\n\\n### Dynamic tags\\n\\nTags are no longer limited to `project/family/group`. You can define any keys that make sense for your organization:\\n\\n```yaml\\napp:\\n  tags:\\n    project: myproject\\n    team: backend\\n    region: eu-west-1\\n    cost_center: engineering\\n```\\n\\nTags are **only required if your naming patterns reference them** \u2014 a minimal setup with `{app.env}-{queueId}` needs no tags at all.\\n\\n---\\n\\n## Naming patterns: define once, use everywhere\\n\\nEvery resource gosoline manages has a configurable naming pattern. Patterns are plain strings with `{placeholder}` macros resolved from your Identity at startup.\\n\\n### The namespace pattern\\n\\n`app.namespace` is the cornerstone of the new naming system. Define your hierarchy once, reference it everywhere:\\n\\n```yaml\\napp:\\n  namespace: \\"{app.tags.project}.{app.env}.{app.tags.family}.{app.tags.group}\\"\\n```\\n\\nWhen `{app.namespace}` appears in a resource pattern, the dots are replaced by that service\'s delimiter \u2014 `-` for most AWS services, `/` for CloudWatch, `_` for Prometheus. This means the same namespace definition produces correctly formatted names for every service automatically.\\n\\n### Concrete example: SQS queues\\n\\n```yaml\\napp:\\n  name: order-service\\n  env: production\\n  namespace: \\"{app.tags.project}.{app.env}.{app.tags.group}\\"\\n  tags:\\n    project: logistics\\n    group: platform\\n\\ncloud:\\n  aws:\\n    sqs:\\n      clients:\\n        default:\\n          naming:\\n            queue_pattern: \\"{app.namespace}-{queueId}\\"\\n            queue_delimiter: \\"-\\"\\n```\\n\\n| Queue ID in code | Resolved queue name |\\n| :--- | :--- |\\n| `orders` | `logistics-production-platform-orders` |\\n| `shipments` | `logistics-production-platform-shipments` |\\n\\nSwitching to `env: dev` automatically updates all queue names \u2014 no queue-specific config changes needed.\\n\\n### Strict placeholder validation\\n\\nUnknown placeholders in naming patterns now return an error at startup. A typo like `{app.tag.project}` (missing `s`) or a leftover legacy `{project}` is caught immediately rather than silently producing wrong resource names in production.\\n\\n### Full pattern reference\\n\\nFor a complete list of all configurable patterns across every service, see the [Naming Patterns reference](/reference/naming-patterns) and [Naming Patterns fundamentals](/fundamentals/naming-patterns).\\n\\n---\\n\\n## `application.Default()` slimmed down\\n\\nPreviously, `application.Default()` bundled ~22 options \u2014 health checks, metrics, tracing, profiling, Sentry, task runner, producer daemon, and more \u2014 whether your application needed them or not. A simple CLI tool or a lightweight worker ended up opting out of most of them, or unknowingly carrying capabilities it didn\'t use.\\n\\n`application.Default()` now ships only the essentials: config loading infrastructure and logger wiring. Everything else \u2014 health checks, metrics, tracing, profiling, stream infrastructure \u2014 is an explicit opt-in. You compose exactly the application you need:\\n\\n```go\\napplication.Run(\\n    // load config from file\\n    application.WithConfigFile(\\"./config.dist.yml\\", \\"yml\\"),\\n\\n    // only the capabilities this service actually uses\\n    application.WithHttpHealthCheck,\\n    application.WithMetrics,\\n    application.WithTracing,\\n\\n    application.WithModuleFactory(\\"my-module\\", NewMyModule),\\n)\\n```\\n\\nA service that only needs config loading and logging needs nothing beyond the new defaults. A full-featured service adds exactly the options it requires \u2014 no hidden defaults to opt out of.\\n\\nThis pairs naturally with the new naming system: since tags are optional and `app.env` defaults to `dev` and `app.name` defaults to `gosoline`, a minimal application needs zero configuration to start. Getting something running no longer requires wiring up a full config file first.\\n\\n---\\n\\n## Other breaking changes\\n\\n### Go API\\n\\n- `cfg.AppId` \u2192 `cfg.Identity`; `cfg.GetAppIdFromConfig()` \u2192 `cfg.GetAppIdentity()`\\n- `WithLoggerGroupTag` \u2192 `WithLoggerApplicationTag(\\"group\\")`; `WithLoggerApplicationTag` \u2192 `WithLoggerApplicationName`\\n\\n### Config key renames\\n\\nPattern keys have been renamed from the generic `naming.pattern` to descriptive, service-specific names: `naming.queue_pattern`, `naming.topic_pattern`, `naming.stream_pattern`, `naming.table_pattern`, etc. Each has an accompanying `naming.*_delimiter` setting.\\n\\n### ModelId refactoring\\n\\n`mdl.ModelId` no longer has explicit hierarchy fields (`.Project`, `.Family`, `.Group`). Like `cfg.Identity`, it now uses a `.Tags` map:\\n\\n```go\\n// Before\\nmodelId := mdl.ModelId{\\n    Project: \\"my-project\\",\\n    Family:  \\"my-family\\",\\n    Name:    \\"my-model\\",\\n}\\n\\n// After\\nmodelId := mdl.ModelId{\\n    Name: \\"my-model\\",\\n    Tags: map[string]string{\\n        \\"project\\": \\"my-project\\",\\n        \\"family\\":  \\"my-family\\",\\n    },\\n}\\n```\\n\\nThe string representation of a `ModelId` \u2014 used for message routing attributes and `mdlsub` publishers \u2014 is now configurable via `app.model_id.domain_pattern`. The model name is always appended automatically as the last dot-separated segment:\\n\\n```yaml\\napp:\\n  model_id:\\n    domain_pattern: \\"{app.tags.project}.{app.env}\\"\\n# Result: myproject.production.myModel\\n```\\n\\nA few things to be aware of:\\n\\n- The `{modelId}` placeholder is **not** used in this pattern \u2014 the model name is appended automatically.\\n- If `domain_pattern` is not configured, calling `modelId.String()` will return an error.\\n- When parsing a canonical model ID string back, each placeholder matches non-dot characters, and the model name is everything after the final dot. This means using non-dot delimiters (e.g. `{app.tags.project}-{app.env}`) is valid, but the last `.` in the full string always marks the boundary before the model name.\\n\\nSee the [Naming Patterns reference](/reference/naming-patterns#modelid-domain-pattern-canonical-ids) for the full rules and examples.\\n\\n### Removed packages\\n\\n- **`pkg/es/`** \u2014 Elasticsearch client integration\\n- **`pkg/parquet/`** \u2014 Parquet file read/write support\\n\\n---\\n\\n## Migration quick start\\n\\nFor full step-by-step instructions covering stream input/output config, `mdlsub` publishers, Go code changes, and per-service pattern updates, see the [migration guide](/migrations/app-identity-and-naming-patterns)."},{"id":"sampling-fingers-crossed-logging","metadata":{"permalink":"/docs/blog/sampling-fingers-crossed-logging","editUrl":"https://github.com/gosoline-project/docs/tree/main/blog/2026-01-15-sampling-fingers-crossed-logging.md","source":"@site/blog/2026-01-15-sampling-fingers-crossed-logging.md","title":"Log Sampling in Go: Less Noise, More Debuggability","description":"High-traffic services have a logging problem: the more successful traffic you handle, the more you pay to store and query logs that rarely matter. But if you turn logging down, you lose the context you need when something does break.","date":"2026-01-15T00:00:00.000Z","tags":[{"inline":true,"label":"logging","permalink":"/docs/blog/tags/logging"},{"inline":true,"label":"sampling","permalink":"/docs/blog/tags/sampling"},{"inline":true,"label":"observability","permalink":"/docs/blog/tags/observability"}],"readingTime":9.99,"hasTruncateMarker":true,"authors":[{"name":"Jan Kamieth","url":"https://github.com/j4k4","imageURL":"https://avatars.githubusercontent.com/u/783502?s=400&v=4","key":"jaka","page":null}],"frontMatter":{"slug":"sampling-fingers-crossed-logging","title":"Log Sampling in Go: Less Noise, More Debuggability","authors":["jaka"],"tags":["logging","sampling","observability"]},"unlisted":false,"prevItem":{"title":"Identity & Naming Patterns: Flexible, Configuration-Driven Resource Naming","permalink":"/docs/blog/identity-and-naming-patterns"}},"content":"High-traffic services have a logging problem: the more successful traffic you handle, the more you pay to store and query logs that rarely matter. But if you turn logging down, you lose the context you need when something *does* break.\\n\\n{/* truncate */}\\n\\n## Why traditional log sampling isn\u2019t enough\\n\\nMany production setups already try to reduce cost by doing *log pipeline sampling* in tools like Fluentd/Fluent Bit, Logstash, or the logging backend itself.\\n\\nThis approach usually looks like \u201Cdrop X% of log lines\u201D or \u201Ckeep 1 out of N entries\u201D, often applied uniformly across an application or per log level.\\n\\nThat can help with volume, but it has two fundamental drawbacks:\\n\\n- **No context-based sampling**: pipeline sampling typically operates on individual log events. It doesn\u2019t understand that a set of logs belong to the same HTTP request, stream message, or job run. You can easily end up keeping the exception log but dropping the ten lines that explain *why* it happened.\\n- **No fingers-crossed behavior**: pipeline sampling drops data permanently. It can\u2019t buffer the \u201Cboring but useful if it fails\u201D debug logs and only emit them when a failure occurs.\\n\\nIn other words: traditional sampling reduces cost, but it tends to reduce *debuggability* at the same time.\\n\\nThe goal of the feature set in this post is to reduce log volume while **keeping failure context intact**.\\n\\nA good logging system should give you:\\n\\n- Low log volume for routine success paths\\n- Full, high-fidelity context for failures\\n- A way to \u201Cturn up\u201D verbosity for a small, controlled slice of traffic\\n- Consistent behavior across HTTP requests, background jobs, and message consumers\\n\\nThis post walks through two techniques that work especially well together:\\n\\n1. **Sampling** \u2014 make a consistent sampled/not-sampled decision and store it in `context.Context`.\\n2. **Fingers-crossed logging** \u2014 buffer logs and only flush them when an error occurs.\\n\\nThe examples use gosoline (a Go framework for cloud microservices), but the patterns are transport-agnostic and apply to any Go service architecture.\\n\\n---\\n\\n## The two ideas\\n\\n### 1) Sampling is a decision carried in context\\n\\nSampling isn\u2019t \u201Crandomly drop log lines\u201D. It\u2019s a *decision about a unit of work*:\\n\\n- Sampled: treat this request/message/job as \u201Cdebuggable\u201D, allow verbose logs\\n- Not sampled: keep things quiet unless there is a failure\\n\\nThe important part: store that decision in `context.Context`, so anything downstream can read it.\\n\\nIn gosoline, this lives in the `smplctx` package.\\n\\n### 2) Fingers-crossed logging buffers context and only emits on failure\\n\\n\u201CFingers-crossed\u201D logging means:\\n\\n- Buffer `debug/info/warn` messages during a scope\\n- If the scope ends successfully: discard the buffer\\n- If an error happens: flush the full buffer (the story leading up to the error)\\n\\nThat gives you rich \u201Cwhat happened before it failed?\u201D context\u2014without paying for it on every success.\\n\\n---\\n\\n## The behavior model (simple mental map)\\n\\nThink in terms of *two* questions:\\n\\n1) Is this unit of work marked \u201Csampled\u201D?  \\n2) Did it fail?\\n\\nIf it\u2019s sampled, you log as usual.\\nIf it\u2019s not sampled, you buffer; if it fails, you flush.\\n\\n---\\n\\n## How to enable it in a gosoline service\\n\\n### How to configure sampling\\n\\nThere are two separate things to set up:\\n\\n- **How decisions are made** (strategies): controlled by config (`sampling.enabled`, `sampling.strategies`, and strategy-specific settings).\\n- **What gosoline does with that decision** (behavior): enabled via an application option (next section).\\n\\nThis separation is deliberate: it lets you define and tune your sampling strategies without changing code, while still keeping sampling-dependent behavior opt-in at the application wiring level.\\n\\nHere\u2019s a minimal config that enables sampling and selects strategies:\\n\\n```yaml\\nsampling:\\n  enabled: true\\n  strategies:\\n    - tracing\\n```\\n\\n---\\n\\n### How to enable sampling in the application\\n\\nThe config above only describes **how to decide** whether something should be sampled.\\n\\nTo actually **activate sampling-aware behavior** (fingers-crossed logging and sampling propagation across messages), enable it in the application wiring:\\n\\n```go\\napp.Run(\\n  // ...\\n  app.WithSampling,\\n)\\n```\\n\\nWhy is this a separate switch?\\n\\n- It keeps the behavior opt-in. Many services may want to create sampling decisions (e.g. for metrics or tracing consistency) without changing logging/stream behavior.\\n- It lets you roll out safely: you can ship configs ahead of time, then enable the new behavior explicitly in code when you\u2019re ready.\\n\\nWhat `app.WithSampling` does at runtime:\\n\\n- **Logger integration**: enables sampling-aware logging so the logger can react to `sampled=true|false` on the context.\\n- **Stream integration**: propagates the sampling decision as a message attribute so consumers can keep consistent behavior across service boundaries.\\n\\n---\\n\\n### Strategy guide (how strategies decide)\\n\\n#### `tracing`: follow upstream trace sampling (X-Ray / OpenTelemetry)\\n\\nIn modern setups, incoming requests often already carry a sampling decision as part of distributed tracing.\\n\\n- **AWS X-Ray** propagates trace context via the `X-Amzn-Trace-Id` header. The header can include a sampling decision (for example, `Sampled=1`), indicating that this request should be traced.\\n- **OpenTelemetry** commonly propagates trace context via the W3C `traceparent` header (and optionally `tracestate`). In W3C propagation, the *trace flags* include a \u201Csampled\u201D bit.\\n\\nThe `tracing` sampling strategy reuses that existing decision:\\n\\n- If there is valid tracing information on the context, gosoline treats the trace sampling flag as the source of truth.\\n- This keeps logs and traces aligned: if a request is traced (sampled), you typically also want its logs to be available immediately and in higher detail.\\n\\nThis is especially useful when your sampling decision is made upstream (e.g., an ingress, API gateway, or service mesh) and you want every downstream service to follow the same \u201Csampled vs. not sampled\u201D choice.\\n\\n#### `probabilistic`: guaranteed baseline + extra traffic\\n\\nIf you don\u2019t have tracing (or don\u2019t want to couple sampling to tracing), a probabilistic strategy is a good default.\\n\\nIt guarantees at least a small amount of sampled traffic per time window (so you always have some detailed examples), and then optionally samples additional requests at a configured percentage.\\n\\nConfiguration example:\\n\\n```yaml\\nsampling:\\n  enabled: true\\n  strategies:\\n    - probabilistic\\n  settings:\\n    probabilistic:\\n      interval: 1s\\n      fixed_sample_count: 1\\n      extra_rate_percentage: 5\\n```\\n\\nInterpretation:\\n\\n- guarantee at least `fixed_sample_count` sampled decisions per `interval`\\n- additionally sample `extra_rate_percentage`% of other traffic\\n\\nThis is a nice operational compromise: you always have *some* detailed traffic, and you can raise/lower the extra percentage depending on cost and debugging needs.\\n\\n### multiple strategies\\n\\nYou can also combine multiple strategies. A common pattern is:\\n\\n- Use `tracing` when trace context exists.\\n- Fall back to `probabilistic` when there is no trace (so you still get a small, controlled amount of sampled traffic).\\n\\nExample:\\n\\n```yaml\\nsampling:\\n  enabled: true\\n  strategies:\\n    - tracing\\n    - probabilistic\\n  settings:\\n    probabilistic:\\n      interval: 1s\\n      fixed_sample_count: 1\\n      extra_rate_percentage: 5\\n```\\n\\n---\\n\\n### How the sampling decision is forwarded downstream in stream messages\\n\\nIn real systems, the most painful sampling failures happen at service boundaries:\\n\\n- Service A decides \u201Cthis request is sampled\u201D (maybe because tracing says so).\\n- Service A publishes an event.\\n- Service B consumes the event\u2026 and makes its *own* sampling decision.\\n\\nIf Service B decides differently, you end up with inconsistent observability: a trace might exist for the end-to-end operation, but the logs are only high-fidelity in some of the services.\\n\\nTo avoid that, gosoline can **propagate the sampling decision via stream message metadata**.\\n\\nWhen `app.WithSampling` is enabled, gosoline adds a default stream encode handler that:\\n\\n- Reads the current sampling decision from the context.\\n- Writes it to a message attribute named `sampled`.\\n\\nOn the consumer side, gosoline restores that decision from the incoming message attribute and places it on the consumer\u2019s `context.Context`. That restored decision should take precedence over locally configured strategies.\\n\\nThe effect is that a single upstream \u201Csampled vs not sampled\u201D choice (often driven by tracing) stays consistent across the entire chain of producers and consumers, without downstream services having to re-decide or accidentally choose differently.\\n\\n---\\n\\n## How fingers-crossed works in HTTP servers and stream consumers\\n\\nFingers-crossed is easiest to reason about if you think of it as **request/message-scoped buffering**:\\n\\n- A scope provides a buffer \u201Cattached\u201D to the `context.Context`.\\n- While the scope is active and the context is **not sampled**, non-error log calls are collected in that buffer.\\n- When a failure signal happens, the buffer is flushed in order, so you see the full lead-up.\\n\\n### HTTP server behavior\\n\\nIn gosoline\u2019s `httpserver`, the middleware chain is set up so that:\\n\\n1. A sampling decision is made early in the request lifecycle.\\n2. The logging middleware establishes a fingers-crossed scope for the request context.\\n3. At the end of the request, gosoline decides whether to flush based on the **HTTP status code**.\\n\\nWhat you get in practice:\\n\\n- **Sampled request** (`sampled=true`): logs are written immediately (no buffering).\\n- **Not sampled request** (`sampled=false`): non-error logs are buffered.\\n- **If an error is logged**: the buffer is flushed immediately, so you get the lead-up right when the first error happens.\\n- **Otherwise, at the end of the request**:\\n  - **status < 400**: buffered logs are discarded (you don\u2019t pay for successful traffic).\\n  - **status >= 400**: buffered logs are flushed, giving you the debug trail for failed requests.\\n\\nThis makes failures \u201Cspeak up\u201D with context, without turning normal traffic noisy.\\n\\n### Stream consumer behavior\\n\\nStream consumers don\u2019t have an HTTP status code, so they need a different \u201Cfailure signal\u201D. In gosoline, the model is:\\n\\n- Consumers restore the sampling decision from the incoming message (attribute `sampled`) when present.\\n- When the context is **not sampled**, gosoline buffers non-error logs while processing the message.\\n- The buffer is flushed when an **error-level log** happens, or when you explicitly call `log.FlushFingersCrossedScope(ctx)`.\\n\\nPractical implications:\\n\\n- For a successful message, not-sampled logs stay quiet.\\n- For a failing message, the error log causes the preceding buffered info/debug logs to be emitted as well.\\n\\nThat gives you \u201Cquiet success, loud failure\u201D behavior across both HTTP and messaging, while still keeping decisions consistent end-to-end.\\n\\n---\\n\\n## How to use \u201Cfingers-crossed\u201D logging manually (jobs, CLIs, workers)\\n\\nFor non-HTTP code (CLI, cron job, custom worker), you can wrap work in a fingers-crossed scope:\\n\\n```go\\nctx := context.Background()\\n\\n// Mark this work as not sampled, to demonstrate buffering behavior.\\nctx = smplctx.WithSampling(ctx, smplctx.Sampling{Sampled: false})\\n\\n// Create a buffer scope. Logs won\u2019t emit immediately.\\nctx = log.WithFingersCrossedScope(ctx)\\n\\nlogger.Info(ctx, \\"starting job: %s\\", jobID)\\nlogger.Debug(ctx, \\"details: %+v\\", payload)\\n\\n// When an error is logged, the buffer flushes.\\nif err := doWork(ctx); err != nil {\\n  logger.Error(ctx, \\"job failed: %w\\", err)\\n  return\\n}\\n```\\n\\nIf you want explicit control, you can flush manually:\\n\\n```go\\nlog.FlushFingersCrossedScope(ctx)\\n```\\n\\n---\\n\\n## How to do HTTP request sampling (including a \u201Cforce sample\u201D override)\\n\\nIf you want a runnable example, see [`examples/httpserver/sampling-fingers-crossed`](https://github.com/justtrackio/gosoline/tree/main/examples/httpserver/sampling-fingers-crossed).\\n\\nAt runtime, you often want a quick escape hatch: \u201Csample this one request\u201D or \u201Cdon\u2019t sample this request\u201D.\\n\\nGosoline\u2019s HTTP server installs sampling middleware early, and it supports an override header:\\n\\n- `X-Goso-Sampled: true` or `false` (parsed as a boolean)\\n\\nSo you can do:\\n\\n```bash\\ncurl -H \'X-Goso-Sampled: true\' https://service.example/api/expensive-call\\n```\\n\\nIn sampled requests, logs behave normally (written immediately).\\n\\nIn not-sampled requests, gosoline uses a fingers-crossed scope: logs are buffered and then flushed automatically when the request fails (HTTP status `>= 400`). This is particularly effective for 500s: you get the full pre-error story without turning the whole service noisy.\\n\\n---\\n\\n## Rollout guidance (practical tips)\\n\\nA few things that make this go smoothly:\\n\\n- Start with low sampling rates (or tracing-based sampling), then tune.\\n- Decide what \u201Cfailure\u201D means for your system. For HTTP it\u2019s natural to flush on `>= 400`, but if your APIs return 4xx for expected control-flow, you may want to review that behavior and/or your status conventions.\\n- Treat sampling as an *observability strategy*, not just a logging trick: it works best when logs, tracing, and message processing all use the same \u201Csampled/not-sampled\u201D decision.\\n\\n---\\n\\n## Closing: why this is worth it\\n\\nThis combo is a good fit for Go services because it leans into Go\u2019s strengths:\\n\\n- `context.Context` is already the idiomatic carrier of request-scoped state\\n- middleware boundaries (HTTP, consumers, jobs) are natural points to decide and propagate sampling\\n- you can keep production cheap and quiet, while still having deep debugging data when it matters"}]}}')}}]);